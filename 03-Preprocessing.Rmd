# Data Preprocessing

* Data Preprocessing must be performed for the **reference set**, used to create the models, as well as the dataset you are making predictions from, the **prediction set**^[prediction set preprocessing skips step 4.5 Select Calibration Set]   
* The prediction set and reference set will be the same dataset if you are just using a **single spectral library**, but can be different if you are using models from one set to make predictions on another.
* Data preprocesing is executed by the functions within `preprocess_functions.R`    
* The code shown below is the section of the `RUNFILE.R` script which calls the preprocessing functions to preprocess the reference set.     * Subsections 4.1-4.4 in this guide explain the steps and their respective functions in more detail    


```{r, eval=FALSE}
#----------------------------------------------#
# Reference Set Preprocessing #
#----------------------------------------------#
source("Functions/metafunctions.R")

# Get Spectral Library Set
refSet_all <- getSpecLib(SAVENAME="refset.ALL")
View(refSet_all)

# Split into Calibration and Validation Sets
split <- calValSplit(SPECLIB=refSet_all) 
refSet <- split$calib; predSet <- split$valid

# Refine Spectral Library
refSet <- subsetSpecLib(SPECLIB=refSet, PROP="OC", SAVENAME="refSet.OC") # Reference Set

```


## Extract OPUS files

For Bruker Instruments, an **OPUS file** containing spectral data, will be output for each sample that is scanned. To compile these separate files into one dataset, we use a couple functions from the `simplerspc` ^[https://github.com/philipp-baumann/simplerspec] package by Philip Baumann, as well as the `stringr` and `foreach` packages.     

Executed within `preprocess_functions.R` :     

* `opus_to_dataset( SPECPATH, NWAVE, SAVE )`     
    + `SPECPATH`: *string*- The path to the folder of opus files, from within the 'Soil-Predictions-Example' folder. Default set to "/Data_Raw/SPECTRA"
    + `NWAVE`: *integer*- The number of wavelengths to extract. This will be set on the FTIR before running. The default is set to 3017, which we use at WHRC
    + `SAVE`: *boolean*- Whether or not you would like to save the extracted spectra as a dataframe. Saved to "Data_Preprocessed/Data_Processed/ref-spectra_original.RData" Default is FALSE     
_  

1. Load appropriate packages for `opus_to_dataset()`...
```{r eval=FALSE}
#---Packages---#
library(stringr) #used for str_sub
library(foreach) #used within read-opus-universal.R
source("Functions/gather-spc.R") #simplerspec function
source("Functions/read-opus-universal.R") #simplerspec function
```
2. Get the paths of all OPUS files...     
A single path will look something like this:    `/Soil-Predictions-Example/Data_Raw/ref-SPECTRA/WHRC03405_S_001_030.0`    
```{r, eval=FALSE}
#---List Files---#
spectraPath <- "/Data_Raw/ref-SPECTRA" #folder of OPUS files
dirs <- list.dirs(paste(getwd(),spectraPath,sep=""), full.names=TRUE)
all.files <- list.files(dirs, pattern= "*.0", recursive=TRUE,full.names=TRUE)
```

3. Extract the spectra and gathers it into a tibble data frame...
```{r eval=FALSE, echo=TRUE, warning=FALSE, collapse=TRUE}
#---Extract Spectra---#
spc_list <- read_opus_univ(fnames = all.files, extract = c("spc"))
soilspec_tbl <- spc_list %>%
  gather_spc()
spc <- soilspec_tbl$spc
```

4. Truncate the dataset to the number of wavelengths specified, to ensure the spectra from different samples align...
```{r eval=FALSE}
spc <- lapply(1:length(spc),function(x) spc[[x]][,1:NWAVE])
```

5. Processe spectra into a dataframe and assigns a `sample_id,` based off the file names ^[sample_ids that are numeric may cause issues while merging so a string ID is advised]...    
```{r eval=FALSE}
spc.df <- as.data.frame(matrix(unlist(spc), nrow=length(spc), byrow=T))
colnames(spc.df) <- colnames(spc[[1]])
spc.df <- data.frame(sample_id = soilspec_tbl$sample_id, spc.df)
spc.df$sample_id <- str_sub(spc.df$sample_id,1,9)
```

6. Optionally save the spectra as an R dataset and csv file...
```{r eval=FALSE}
if(SAVE==TRUE){
    save(spectra, file="Data_Processed/ref-spectra_original.RData")
    write.csv(spectra, "Data_Processed/ref-spectra_original.csv", row.names=FALSE)
}
```

## Process Spectra

### Subset Spectral Range {-}
Executed within `preprocess_functions.R` :    

* `subset_spectral_range( SPECTRA )`    
    + `SPECTRA`: *matrix*- A matrix of spectral data

Narrow down the regions of the spectra by truncating wavenumbers below 628 and between 2268 to 2389, which is a CO2 sensitive region
```{r eval=FALSE}
#---Edit Spectral Columns---#
col.names <- colnames(spectra$spc) #get column names which are wavenumbers
col.names <- as.numeric(substring(col.names,2))

cutoff <- which(col.names <= 628)[1]
spectra$spc <- spectra$spc[,-c(cutoff:length(col.names))] #truncate at >= 628

min.index <- which(col.names <= 2389)[1]
max.index <- which(col.names <= 2268)[1]
spectra$spc <- spectra$spc[,-c(min.index:max.index)] #remove CO2 region
```

### Baseline Transformation {-}
Executed within `preprocess_functions.R` :     

* `base_offset(x)`
    + `x`: a matrix

Perform a baseline transformation to normalize the spectra, by subtracting the minimum values for each row/sample.
```{r eval=FALSE}
library(matrixStats) # Used for rowMins() function
base_offset <- function(x){
  row_mins <- rowMins(x)
  return(x-row_mins) # Subtracts row_mins
}
```

<!--could show head of the spectra, or a figure showing the dimensions-->
### Other Transformations {-}
* Standard Normal Variate
* First Derivative

### (Calibration Transfer) {-}
{Optional}
Recommended when the spectral library of samples to be predicted was scanned by a different instrument than the samples used to built the model.
For example, you would want to perform a calibration transfer on the prediction set, if you were using the KSSL library to make predictions on samples scanned at Woods Hole Research Center.


## Merge with Lab Data
If there is lab data associated with your soil samples, this can be merged with the spectral data and later used to assess the performance of your models. The example lab dataset below provides information about where the soil sample was taken with the Site_ID and Horizon, as well as the lab measurements for various soil properties including Organic Carbon, Sand, Silt and Clay.

<!--discuss FTIR run-lists and associating IDs -->
```{r eval=FALSE}
#---Read Lab Data---#
lab <- data.frame(read.csv("Data_Raw/ref-LAB_DATA.csv"))
```
            
            
The merge() command joins the lab dataset to the spectral dataset. The all.y=TRUE parameter indicates that the final dataset will contain all the rows of spectra. This means that if some samples do not have lab data, they will be assigned a value of NA but the spectra will remain in the set. 
```{r eval=FALSE}
#---Merge Data---#
all_data <- merge(lab, spectra, all.y=TRUE)
```

Save the reference set file after preprocessing...
```{r eval=FALSE}
#---Save after Preprocessing---#
if(file.exists("./Data_Processed")==FALSE){dir.create("./Data_Processed")}
save(all_refset, file="Data_Processed/refset.ALL.RData")
write.csv(all_refset, "Data_Processed/refset.ALL.csv", row.names=FALSE)
```

The final dataframe contains a unique ID, lab data, and a matrix of spectral data called 'spc'. It is suggested to save this file as RData so it may be reloaded as needed.    

![ref.ALL.RData](images/ref.ALL.png)  

## (Refine Reference Set)

If you are preprocessing the reference set, you may want to refine the samples you use to build your models by...     

* Subsetting the whole reference set to 15000 samples    
* Eliminating samples with NA, negative, or outlier lab data      
* Spliting the set or property subsets into calibration and validation groups    

### Large Sets {-}
If you reference set exceeds 15000 samples, you may chose to subset it. We have found that 15000 is optimal for speed and performance of the models, when the reference set is very large. This subset can be performed using **conditional latin hypercube sampling**, with the `clhs` package. 

#### Functions {-}    

Executed within `preprocess_functions.R` :    

* `sub_large_set( dataset, subcount)`
    + `dataset`: *dataframe*- Dataframe including spectral data as a matrix 'spc'
    + `subcount`: *integer*- Number of samples to subset. Default is set to 15000    
    
```{r, eval=FALSE}
library(clhs)
sub_large_set <- function(dataset, subcount=15000){
  spectra <- data.frame(dataset$spc)
  subset <- clhs(spectra, size = subcount, progress = TRUE, iter = 500)
  dataset <- dataset[subset,] #double check
  return(dataset)
}
```

### Faulty Lab Data {-}
To yield the best models, you will want to exclude rows with faulty lab data (NA, negative, and outlier values). This may vary by soil property, so you should repeat this process separately and save separate reference sets for each property (ie. ref.OC.RData)    

#### NA Values {-}

Executed within `preprocess_functions.R` :

* `noNA( dataset, column )`
    + `dataset`: dataframe to eliminate NAs from
    + `column`: column to check for NA values   
    
Gets rid of NA values...
```{r, eval=FALSE}
noNA <- function(dataset, column){
  return(dataset[!is.na(dataset[,column]),])
}
```

#### Negative Values {-}

* `noNeg( dataset, column )`
    + `dataset`: dataframe to eliminate negative values from
    + `column`: column to check for negative values   

Gets rid of Negative values...
```{r, eval=FALSE}
noNeg <- function(dataset, column){
  return(dataset[which(dataset[,column] > 0),])
}
```

### Outliers {-}

* `noOut( dataset, column )`
    + `dataset`: dataframe to eliminate outliers from
    + `column`: column to check for outliers   
    
Gets rid of Outliers...
```{r, eval=FALSE}
source("Functions/outlier_functions.R")
noOut <- function(dataset, column){
  outliers <- stdev_outliers(dataset, column)
  return(dataset[-outliers,])
}
```

#### Standard Deviation (Lab Data Outliers) {-}

The `noOut()` function (above) calls the `stdev_outliers()` function (below), which is within `outlier_functions.R`. This outlier detection approach creates a PLS model, and identifies predictions that were that exceed a certain standard deviation threshold. {come back to for more detail}

Executed within `outlier_functions.R` :

* `stdev_outliers( dataset, column )`
    + `dataset`: dataframe to eliminate standard deviation outliers from
    + `column`: column to check for standard deviation outliers 
    
``` {r, eval=FALSE}
stdev_outliers <- function(dataset, column){
  
  # Create a PLS model with the data
  pls.fit <- plsr(sqrt(get(column))~spc, ncomp= 20, data = dataset, valid="CV", segments = 50) #y, x, number components, data, cross validation, 
  pred <- c(predict(pls.fit, newdata = dataset$spc,ncomp=20))^2
  
  # Identify outliers using a standard deviation threshold
  sd.outlier <- optimum_sd_outlier(pred, dataset[,column], seq(0.1,3, by =0.02))
  outlier.indices <- outlier_indices(pred, dataset[,column], sd.outlier[1])

  return(outlier.indices)
}
```

#### Fratio (Spectral Outliers) {-}

#### Script {-}    

Sets the properties you would like make references sets for...
```{r, eval=FALSE}
# Remove rows with poor lab data
properties <- c("OC","SAND","SILT", "CLAY") #Column names of lab data
```

Gets rid of NA, negative and outlier values and saves subsets...
```{r, eval=FALSE}
for(property in properties){
  prop_refset <- all_refset
  prop_refset <- noNA(prop_refset, property) # Remove NAs
  prop_refset <- noNeg(prop_refset, property) # Remove Negative
  prop_refset <- noOut(prop_refset, property) # Remove Outliers*
  savename <- paste("refset", property, sep=".") # Ex: refset.OC
  assign(savename, prop_refset)
  save(list= savename, file=paste0("Data_Processed/", savename, ".RData"))
}
```

<!--Insert plot showing the outlier selection process-->

### Cal/Val Groups {-}

You may chose to subset a portion of the reference set as a calibration group which will be used to build the models- leaving the remaining samples as the validation set to test the model.

Kennard Stone is a method for performing this type of separation while ensuring each group is representative of the set.^[This step can be skipped if using MBL modeling approach which uses the entire dataset. If both modeling approaches are being used, you can load and row bind the calibration and validation sets for MBL]

#### Functions {-}
Executed within `preprocess_functions.R` :

* `calValSplit( dataset, column )`
    + `dataset`: *dataframe*- The reference dataset with a 'spc' column containing the spectral matrix    

```{r, eval=FALSE}  
library(prospectr)
calValSplit <- function(dataset){
  #perform kennard stone to separate data into 80% calibration and 20% validation sets
  ken_stone<- prospectr::kenStone(X = dataset$spc, 
                                  k = as.integer(0.8*nrow(dataset)), 
                                  metric = "mahal", pc = 10)
  calib <- dataset[ken_stone$model, ]
  valid <- dataset[ken_stone$test, ]
  
  return(c(calib, valid))
}
```

<!--An alternative to saving the datasets would be to add a column indicating whether a sample falls under the calibration or validation set. This could be used to subset the correct datasets when making predictions. If the full dataset is large and takes a while to load, saving the former approach is preferred. If the set loads relatively quickly, it may be worth it to save storage space and use the latter method.-->

#### Script {-}

To split the reference set into calibration and validation groups...
```{r, eval=FALSE}
split <- calValSplit(all_refset, property) 
calib <- split[1]; valid <- split[2]
save(calib, file="Data_Processed/calib.ALL.RData")
save(valid, file="Data_Processed/valid.ALL.RData")
```


To split the property subsets into calibration and validation groups...
```{r, eval=FALSE}
properties <- c("OC","SAND","SILT", "CLAY") #Column names of lab data
for(property in properties){
  
  # Remove
  prop_refset <- all_refset
  prop_refset <- noNA(prop_refset, property) # Remove NAs
  prop_refset <- noNeg(prop_refset, property) # Remove Negative
  prop_refset <- noOut(prop_refset, property) # Remove Outliers*
  savename <- paste("refset", property, sep=".") # Ex: refset.OC
  assign(savename, prop_refset)
  save(list= savename, file=paste0("Data_Processed/", savename, ".RData"))
  
  # Split Calibration & Validation Sets
  split <- calValSplit(prop_refset, property) 
  calib <- split[1]; valid <- split[2]
  save(calib, file=paste("Data_Processed/calib", property,"RData", sep="."))
  save(valid, file=paste("Data_Processed/valid", property,"RData", sep="."))
}
```
