[
["index.html", "Soil Predictions using MIR-Spectroscopy 1 Introduction", " Soil Predictions using MIR-Spectroscopy Charlotte Rivard 5/8/2020 1 Introduction This guide documents the use of MIR-spectroscopy at the Woods Hole Research Center, to predict various soil health properties. It is aimed at helping other teams build soil spectral libraries, create models from their data, access model performance against known values, and ultimately make predictions on new data. The machine learning methods outlined in this document were selected after assessing the performance of a variety of models. This process is explained in the following paper: Dangal S.R.S., J. Sanderman, S. Wills, and L. Rameriz-Lopez. 2019. Accurate and Precise Prediction of Soil Properties from a Large Mid-Infrared Spectral Library. Soil Systems 3(1):11. doi:10.3390/soilsystems3010011 Questions and comments can be sent to jsanderman@whrc.org or crivard@whrc.org Woods Hole Research Center Website: whrc.org Instagram: woodsholeresearchcenter Address: 149 Woods Hole Rd, Falmouth, MA 02540 "],
["background.html", "2 Background", " 2 Background “Diffuse reflectance spectroscopy (DRS) has been demonstrated to be a viable alternative for rapidly characterizing and measuring soil properties compared with time-consuming and expensive conventional soil laboratory analysis [6]. Visible (vis; 400–700 nm), near-infrared (NIR; 700–2500 nm), and mid-infrared (MIR; 2500–25,000 nm) regions have been used widely to characterize soil minerals and organic matter at the global [5,7,8], regional [9,10], national [11,12] and local scale [13]. Vis–NIR can be preferable to MIR due to its low instrumental cost and potential for field deployment [14]. Therefore, for soil surveys that require high sampling density with basic soil properties measurement (for, e.g., organic carbon, soil texture), vis–NIR may be preferable. In contrast, the MIR region contains strong molecular vibrations of most important soil minerals and organic components [7,15]. As a result, models built using MIR databases often perform better compared with the vis–NIR database for many soil properties, particularly for more minor soil constituents [6,16]” (soil systems paper) https://www.researchgate.net/figure/Representative-soil-mid-IR-spectrum-showing-absorptions-related-to-the-mineral-and_fig1_282420764 "],
["getting-started.html", "3 Getting Started 3.1 File Walkthrough 3.2 Required Packages 3.3 Full Script", " 3 Getting Started The best way to get started using this code, is by downloading the Soil-Predictions-Example folder found here: Soil-Predictions-Example Folder This folder, along with all source code for this guide, can be found in the following Github Repository: whrc/Soil-Predictions-MIR 3.1 File Walkthrough Within the Soil-Predictions-Example folder, you will find the following folders and files: [1] &quot;Data_Raw&quot; [2] &quot;Functions&quot; [3] &quot;RUNFILE.R&quot; [4] &quot;Soil-Predictions-Example.Rproj&quot; Double click Soil-Predictions-Example.Rproj to open up the R-project. Within a project, the working directory is set to the project’s folder. Open up RUNFILE.R in the project environment. This is an example script of how to make soil predictions using spectral data. It includes the use of both PLSR models and MBL models, which are both explained in this guide. Navigate to the Functions folder. Within this folder are R files containing functions useful for MIR soil predictions. These files will be sourced by each other, and RUNFILE.R [1] &quot;caltransfer_functions.R&quot; [2] &quot;gather-spc.R&quot; # from &#39;simplerspc&#39; package [3] &quot;outlier_functions.R&quot; [4] &quot;plsr_functions.R&quot; [5] &quot;preprocess_functions.R&quot; [6] &quot;read-opus-universal.R&quot; # from &#39;simplerspc&#39; package Navigate to the Data_Raw folder. This should contain: ref-LAB_DATA.csv: A ‘.csv’ file of the lab data; sample_id column and column lab data for a given property, at a minimum. ref-SPECTRA: A folder of OPUS files containing the spectral data for each sample 3.2 Required Packages Open up RUNFILE.R and install the packages listed at the top: #install.packages(readr) #install.packages(pls) #install.packages(&quot;miceadds&quot;) library(miceadds) #used for the load as this variable thing 3.3 Full Script Run the RUNFILE.R script. This will create… Data_Processed: A folder containing the processed data, used to build the model and make predictions Models: A folder containing the plsr and mbl models made. As well as a summary of their performance, model_performance.csv Predictions: A folder containing the predictions output by the script To modify for your own spectral library…. Change the spectral files in Data_Raw/ref-SPECTRA Change the lab data in Data_Raw/ref-LAB_DATA.csv Update the variable properties throughout RUNFILE.R, to contain the column names of the properties in your lab dataset. (Right now it is set to c(“OC”, “SAND”, “SILT”,&quot;CLAY)) Below is the full RUNFILE.R script, organized with 3 main sections: Data Preprocessing PLSR Models MBL Models #Title: RUNFILE- Soil-Predictions-Example #Authors: Charlotte Rivard &amp; Shree Dangal #Date: 5/8/20 #Summary: #The following script predicts values for several soil properties #from spectral data, using the following machine learning models: #1- Partial Least Squares Regression #2- Memory Based Learner #----------------------------------------------# # Install Packages #----------------------------------------------# #install.packages(readr) #install.packages(pls) #install.packages(&quot;miceadds&quot;) library(miceadds) #used for the load as this variable thing #----------------------------------------------# # Reference Set Preprocessing # #----------------------------------------------# source(&quot;Functions/preprocess_functions.R&quot;) # Process Reference Set Spectra spectra &lt;- opus_to_dataset(&quot;/Data_Raw/ref-SPECTRA&quot;) spectra$spc &lt;- subset_spectral_range(spectra$spc) spectra$spc &lt;- base_offset(spectra$spc) # Merge with Reference Set Lab Data library(readr) lab &lt;- data.frame(read_csv(&quot;Data_Raw/ref-LAB_DATA.csv&quot;)) all_refset &lt;- merge(lab, spectra, all.y=TRUE) # Save refset.ALL file after preprocessing if(file.exists(&quot;./Data_Processed&quot;)==FALSE){dir.create(&quot;./Data_Processed&quot;)} save(all_refset, file=&quot;Data_Processed/refset.ALL.RData&quot;) write.csv(all_refset, &quot;Data_Processed/refset.ALL.csv&quot;, row.names=FALSE) # Remove rows with poor lab data properties &lt;- c(&quot;OC&quot;,&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) #Column names of lab data for(property in properties){ prop_refset &lt;- all_refset prop_refset &lt;- noNA(prop_refset, property) # Remove NAs prop_refset &lt;- noNeg(prop_refset, property) # Remove Negative prop_refset &lt;- noOut(prop_refset, property) # Remove Outliers* #prop_refset$spc &lt;- sub_large_set(prop_refset) # Subset to 15000 if large savename &lt;- paste(&quot;refset&quot;, property, sep=&quot;.&quot;) # Ex: refset.OC assign(savename, prop_refset) save(list= savename, file=paste0(&quot;Data_Processed/&quot;, savename, &quot;.RData&quot;)) #split &lt;- calValSplit(prop_refset, property) # Split Calibration &amp; Validation Sets #calib &lt;- split[1]; valid &lt;- split[2] #save(calib, file=paste(&quot;Data_Processed/calib&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) #save(valid, file=paste(&quot;Data_Processed/valid&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) } #----------------------------------------------# # Prediction Set Preprocessing # #----------------------------------------------# # Spectral Processing: Prediction Set #spectra &lt;- opus_to_dataset(&quot;/Data_Raw/PRED-SPECTRA&quot;) #{where pds would happen} #spectra &lt;- subset_spectral_range(spectra) #spectra &lt;- base_offset(spectra) #----------------------------------------------# # Partial Least Squares Regression # #----------------------------------------------# library(pls) source(&quot;Functions/plsr_functions.R&quot;) #------ CREATE MODELS -------# # Create Folder to Save Models if(file.exists(&quot;./Models&quot;)==FALSE){dir.create(&quot;./Models&quot;)} # List Properties to Make Models For properties &lt;- c(&quot;OC&quot;, &quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) for(property in properties){ # Load Data refSetPath &lt;- paste(&quot;./Data_Processed/refset&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;) # Ex: refset.OC.RData load.Rdata(refSetPath, &quot;refSet&quot;) # load as variable refSet # Create Model validType &lt;- &quot;CV&quot; # &quot;CV&quot;, &quot;LOO&quot;, or &quot;none&quot; plsr.model &lt;- plsr(sqrt(get(property))~spc, ncomp=20, data = refSet , valid=validType) # Save Model modelName &lt;- paste(&quot;plsr&quot;, property, sep=&quot;.&quot;) assign(modelName, plsr.model) save(list= modelName, file = paste(&quot;./Models/plsr&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) #Ex: plsr.OC.RData } #------ Apply Models -------# # Load Prediction Set #predSetPath &lt;- &quot;./Data_Processed/refset.ALL.RData&quot; predSetPath &lt;- &quot;./Data_Processed/predset.TEST.RData&quot; load.Rdata(predSetPath, &quot;predSet&quot;) # variable predSet # Load/Create File to Save Predictions predSavePath &lt;- &quot;./Predictions/predset.TEST.predictions.RData&quot; if(file.exists(predSavePath) ){ load( predSavePath) }else{ all_predictions &lt;- predSet[,-ncol(predSet)] # remove spectra, last column } # Make and Save Predictions for(property in properties){ # Load Model (Ex: plsr.OC.RData, variable= plsr.model) model.name &lt;- load(paste(&quot;./Models/plsr&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;), verbose=TRUE) plsr.model &lt;- get(model) # Load Reference Set (Ex: ref.OC.RData, variable= prop.data) load(paste(&quot;./Data_Processed/ref&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;), verbose=TRUE) # Find Optimal Number of Components ncomp_onesigma &lt;- selectNcomp(plsr.model, method = &quot;onesigma&quot;, plot=TRUE, main=paste(property,&quot;Validation&quot;)) # Get Predictions predType &lt;- &quot;predict&quot; # &quot;fitted&quot;, &quot;valid&quot;, &quot;predict&quot; predictions &lt;- getPredictions(plsr.model, ncomp_onesigma, predType, predSet) # Save Predictions #sample_id &lt;- getSampleID(prop.data, get(predSet), predType) # reference set, prediction set, prediction type samp_id &lt;- getSample(property, predType) predTable &lt;- data.frame(sample_id, predictions) names(predTable) &lt;- c(&quot;sample_id&quot;, paste(property, predType, sep=&quot;.&quot;)) all_predictions &lt;- merge(all_predictions, predTable, all.X=TRUE) # Save Model Performance lab_data &lt;- getLabData(plsr.model, predType, predSet, property) # get lab data to compare saveModStats(predictions, lab_data, property, ncomp_onesigma, &quot;PLSR&quot;, predType, predSet) } # Save All Predictions if(file.exists(&quot;./Predictions&quot;)==FALSE){dir.create(&quot;./Predictions&quot;)} save(all_predictions, file=&quot;./Predictions/all_predictions.RData&quot;) write.csv(all_predictions, &quot;./Predictions/all_predictions.csv&quot;, row.names=FALSE) #outlier flagging #output.pred &lt;- data.frame(output.pred) #output.pred$outlier &lt;- 0 #oc &lt;- read.csv(&quot;/mnt/data2/disk1/soilcarbon/crivard/predEnsemble/output/indigo_output/fratio/bd.anal4.csv&quot;) #outs &lt;- c(oc$x) #outs &lt;- outs[outs&gt;10363] #outs &lt;- outs-10363 #output.pred$outlier[outs] &lt;- 1 #output.pred &lt;- cbind(TERR_ID,output.pred) #write.csv(output.pred, file =&quot;/mnt/data2/disk1/soilcarbon/crivard/predEnsemble/output/indigo_output/all-predictions/pred.bd.anal4.csv&quot;) library(miceadds) #Create / Load File to Save Predictions predSet &lt;- get(load(&quot;./Data_Processed/ref.ALL.RData&quot;)) if(file.exists(&quot;./Predictions/all_predictions.RData&quot;)){ load(&quot;./Predictions/all_predictions.RData&quot;, verbose=TRUE) }else{all_predictions &lt;- all_data[,-ncol(all_data)]}# remove spectra, last column for(property in properties){ # Load Model (Ex: plsr.OC.RData, variable= plsr.model) load(paste(&quot;./Models/plsr&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;), verbose=TRUE) # Load Reference Set (Ex: ref.OC.RData, variable= prop.data) load(paste(&quot;./Data_Processed/ref&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;), verbose=TRUE) # Find Optimal Number of Components ncomp_onesigma &lt;- selectNcomp(plsr.model, method = &quot;onesigma&quot;, plot=TRUE, main=paste(property,&quot;Validation&quot;)) # Get Predictions predType &lt;- &quot;predict&quot; # &quot;fitted&quot;, &quot;valid&quot;, &quot;predict&quot; predictions &lt;- getPredictions(plsr.model, ncomp_onesigma, predType, get(predSet)) # Save Predictions sample_id &lt;- getSampleID(prop.data, get(predSet), predType) # reference set, prediction set, prediction type predTable &lt;- data.frame(sample_id, predictions) names(predTable) &lt;- c(&quot;sample_id&quot;, paste(property, predType, sep=&quot;.&quot;)) all_predictions &lt;- merge(all_predictions, predTable, all.X=TRUE) # Save Model Performance lab_data &lt;- getLabData(plsr.model, predType, get(predSet), property) # get lab data to compare saveModStats(predictions, lab_data, property, ncomp_onesigma, &quot;PLSR&quot;, predType, predSet) } # Save All Predictions if(file.exists(&quot;./Predictions&quot;)==FALSE){dir.create(&quot;./Predictions&quot;)} save(all_predictions, file=&quot;./Predictions/all_predictions.RData&quot;) write.csv(all_predictions, &quot;./Predictions/all_predictions.csv&quot;, row.names=FALSE) #----------------------------------------------# # Memory Based Learner Model # #----------------------------------------------# library(miceadds) library(resemble) property &lt;- &quot;OC&quot; #for(property in properties){ # Load Reference Set refSetPath &lt;- paste(&quot;./Data_Processed/refset&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;) # Ex: refset.OC.RData load.Rdata(refSetPath, &quot;refSet&quot;) # load as variable refSet # Load Prediction Set #predSetPath &lt;- &quot;./Data_Processed/refset.ALL.RData&quot; predSetPath &lt;- &quot;./Data_Processed/predset.TEST.RData&quot; load.Rdata(predSetPath, &quot;predSet&quot;) # variable predSet # Load/Create File to Save Predictions predSavePath &lt;- &quot;./Predictions/predset.TEST.predictions.RData&quot; if(file.exists(predSavePath) ){ load( predSavePath) }else{ all_predictions &lt;- predSet[,-ncol(predSet)] # remove spectral matrix } # Set Input Datasets Xu &lt;- predSet$spc Yu &lt;- sqrt(predSet[,property]) Yr &lt;- sqrt(refSet[,property]) Xr &lt;- refSet$spc ctrl &lt;- mblControl(sm = &#39;pc&#39;, pcSelection = list(&#39;opc&#39;, 50), valMethod = &#39;loc_crossval&#39;,center=TRUE,scale=FALSE,allowParallel=FALSE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Yu = Yu, Xu = Xu, mblCtrl = ctrl, dissUsage = &#39;none&#39;, k = seq(40, 100, by = 20), method = &#39;pls&#39;, pls.c = 6) # Get Best Predictions index_best_model &lt;- which.min(mbl.sqrt$localCrossValStats$st.rmse) best_model_name &lt;- names(mbl.sqrt$results)[3] predictions &lt;- eval(parse( text=paste0(&quot;mbl.sqrt$results$&quot;,best_model_name,&quot;$pred&quot; ))) # Save Model Performance lab_data &lt;- predSet[,property] # get lab data to compare saveModStats(predictions, lab_data, property, NA, &quot;MBL&quot;, NA, predSetPath) "],
["data-preprocessing.html", "4 Data Preprocessing 4.1 Extract OPUS files 4.2 Process Spectra 4.3 Merge with Lab Data 4.4 (Refine Reference Set)", " 4 Data Preprocessing Data Preprocessing must be performed for the reference set, used to create the models, as well as the dataset you are making predictions from, the prediction set1 The prediction set and reference set will be the same dataset if you are just using a single spectral library, but can be different if you are using models from one set to make predictions on another. Data preprocesing is executed by the functions within preprocess_functions.R The code shown below is the section of the RUNFILE.R script which calls the preprocessing functions to preprocess the reference set. * Subsections 4.1-4.4 in this guide explain the steps and their respective functions in more detail #----------------------------------------------# # Reference Set Preprocessing # #----------------------------------------------# source(&quot;Functions/preprocess_functions.R&quot;) # Process Reference Set Spectra spectra &lt;- opus_to_dataset(&quot;/Data_Raw/ref-SPECTRA&quot;) spectra$spc &lt;- subset_spectral_range(spectra$spc) spectra$spc &lt;- base_offset(spectra$spc) # Merge with Reference Set Lab Data lab &lt;- data.frame(read.csv(&quot;Data_Raw/ref-LAB_DATA.csv&quot;)) all_refset &lt;- merge(lab, spectra, all.y=TRUE) # Save the Full Reference Set after Preprocessing if(file.exists(&quot;./Predictions&quot;)==FALSE){dir.create(&quot;./Data_Processed&quot;)} save(all_refset, file=&quot;Data_Processed/refset.ALL.RData&quot;) write.csv(all_refset, &quot;Data_Processed/refset.ALL.csv&quot;, row.names=FALSE) # Save Reference Sets Optimal for each Property properties &lt;- c(&quot;OC&quot;,&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) #Column names of lab data for(property in properties){ prop_refset &lt;- all_refset prop_refset &lt;- noNA(prop_refset, property) # Remove NAs prop_refset &lt;- noNeg(prop_refset, property) # Remove Negative prop_refset &lt;- noOut(prop_refset, property) # Remove Outliers* savename &lt;- paste(&quot;refset&quot;, property, sep=&quot;.&quot;) # Ex: refset.OC assign(savename, prop_refset) save(list= savename, file=paste0(&quot;Data_Processed/&quot;, savename, &quot;.RData&quot;)) #split &lt;- calValSplit(prop_refset, property) # Split Calibration &amp; Validation Sets #calib &lt;- split[1]; valid &lt;- split[2] #save(calib, file=paste(&quot;Data_Processed/calib&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) #save(valid, file=paste(&quot;Data_Processed/valid&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) } 4.1 Extract OPUS files For Bruker Instruments, an OPUS file containing spectral data, will be output for each sample that is scanned. To compile these separate files into one dataset, we use a couple functions from the simplerspc2 package by Philip Baumann, as well as the stringr and foreach packages. Executed within preprocess_functions.R : opus_to_dataset( SPECPATH, NWAVE, SAVE ) SPECPATH: string- The path to the folder of opus files, from within the ‘Soil-Predictions-Example’ folder. Default set to “/Data_Raw/SPECTRA” NWAVE: integer- The number of wavelengths to extract. This will be set on the FTIR before running. The default is set to 3017, which we use at WHRC SAVE: boolean- Whether or not you would like to save the extracted spectra as a dataframe. Saved to “Data_Preprocessed/Data_Processed/ref-spectra_original.RData” Default is FALSE _ Load appropriate packages for opus_to_dataset()… #---Packages---# library(stringr) #used for str_sub library(foreach) #used within read-opus-universal.R source(&quot;Functions/gather-spc.R&quot;) #simplerspec function source(&quot;Functions/read-opus-universal.R&quot;) #simplerspec function Get the paths of all OPUS files… A single path will look something like this: /Soil-Predictions-Example/Data_Raw/ref-SPECTRA/WHRC03405_S_001_030.0 #---List Files---# spectraPath &lt;- &quot;/Data_Raw/ref-SPECTRA&quot; #folder of OPUS files dirs &lt;- list.dirs(paste(getwd(),spectraPath,sep=&quot;&quot;), full.names=TRUE) all.files &lt;- list.files(dirs, pattern= &quot;*.0&quot;, recursive=TRUE,full.names=TRUE) Extract the spectra and gathers it into a tibble data frame… #---Extract Spectra---# spc_list &lt;- read_opus_univ(fnames = all.files, extract = c(&quot;spc&quot;)) soilspec_tbl &lt;- spc_list %&gt;% gather_spc() spc &lt;- soilspec_tbl$spc Truncate the dataset to the number of wavelengths specified, to ensure the spectra from different samples align… spc &lt;- lapply(1:length(spc),function(x) spc[[x]][,1:NWAVE]) Processe spectra into a dataframe and assigns a sample_id, based off the file names3… spc.df &lt;- as.data.frame(matrix(unlist(spc), nrow=length(spc), byrow=T)) colnames(spc.df) &lt;- colnames(spc[[1]]) spc.df &lt;- data.frame(sample_id = soilspec_tbl$sample_id, spc.df) spc.df$sample_id &lt;- str_sub(spc.df$sample_id,1,9) Optionally save the spectra as an R dataset and csv file… if(SAVE==TRUE){ save(spectra, file=&quot;Data_Processed/ref-spectra_original.RData&quot;) write.csv(spectra, &quot;Data_Processed/ref-spectra_original.csv&quot;, row.names=FALSE) } 4.2 Process Spectra Subset Spectral Range Executed within preprocess_functions.R : subset_spectral_range( SPECTRA ) SPECTRA: matrix- A matrix of spectral data Narrow down the regions of the spectra by truncating wavenumbers below 628 and between 2268 to 2389, which is a CO2 sensitive region #---Edit Spectral Columns---# col.names &lt;- colnames(spectra$spc) #get column names which are wavenumbers col.names &lt;- as.numeric(substring(col.names,2)) cutoff &lt;- which(col.names &lt;= 628)[1] spectra$spc &lt;- spectra$spc[,-c(cutoff:length(col.names))] #truncate at &gt;= 628 min.index &lt;- which(col.names &lt;= 2389)[1] max.index &lt;- which(col.names &lt;= 2268)[1] spectra$spc &lt;- spectra$spc[,-c(min.index:max.index)] #remove CO2 region Baseline Transformation Executed within preprocess_functions.R : base_offset(x) x: a matrix Perform a baseline transformation to normalize the spectra, by subtracting the minimum values for each row/sample. library(matrixStats) # Used for rowMins() function base_offset &lt;- function(x){ row_mins &lt;- rowMins(x) return(x-row_mins) # Subtracts row_mins } Other Transformations Standard Normal Variate First Derivative (Calibration Transfer) {Optional} Recommended when the spectral library of samples to be predicted was scanned by a different instrument than the samples used to built the model. For example, you would want to perform a calibration transfer on the prediction set, if you were using the KSSL library to make predictions on samples scanned at Woods Hole Research Center. 4.3 Merge with Lab Data If there is lab data associated with your soil samples, this can be merged with the spectral data and later used to assess the performance of your models. The example lab dataset below provides information about where the soil sample was taken with the Site_ID and Horizon, as well as the lab measurements for various soil properties including Organic Carbon, Sand, Silt and Clay. #---Read Lab Data---# lab &lt;- data.frame(read.csv(&quot;Data_Raw/ref-LAB_DATA.csv&quot;)) The merge() command joins the lab dataset to the spectral dataset. The all.y=TRUE parameter indicates that the final dataset will contain all the rows of spectra. This means that if some samples do not have lab data, they will be assigned a value of NA but the spectra will remain in the set. #---Merge Data---# all_data &lt;- merge(lab, spectra, all.y=TRUE) Save the reference set file after preprocessing… #---Save after Preprocessing---# if(file.exists(&quot;./Data_Processed&quot;)==FALSE){dir.create(&quot;./Data_Processed&quot;)} save(all_refset, file=&quot;Data_Processed/refset.ALL.RData&quot;) write.csv(all_refset, &quot;Data_Processed/refset.ALL.csv&quot;, row.names=FALSE) The final dataframe contains a unique ID, lab data, and a matrix of spectral data called ‘spc’. It is suggested to save this file as RData so it may be reloaded as needed. ref.ALL.RData 4.4 (Refine Reference Set) If you are preprocessing the reference set, you may want to refine the samples you use to build your models by… Subsetting the whole reference set to 15000 samples Eliminating samples with NA, negative, or outlier lab data Spliting the set or property subsets into calibration and validation groups Large Sets If you reference set exceeds 15000 samples, you may chose to subset it. We have found that 15000 is optimal for speed and performance of the models, when the reference set is very large. This subset can be performed using conditional latin hypercube sampling, with the clhs package. Functions Executed within preprocess_functions.R : sub_large_set( dataset, subcount) dataset: dataframe- Dataframe including spectral data as a matrix ‘spc’ subcount: integer- Number of samples to subset. Default is set to 15000 library(clhs) sub_large_set &lt;- function(dataset, subcount=15000){ spectra &lt;- data.frame(dataset$spc) subset &lt;- clhs(spectra, size = subcount, progress = TRUE, iter = 500) dataset &lt;- dataset[subset,] #double check return(dataset) } Faulty Lab Data To yield the best models, you will want to exclude rows with faulty lab data (NA, negative, and outlier values). This may vary by soil property, so you should repeat this process separately and save separate reference sets for each property (ie. ref.OC.RData) NA Values Executed within preprocess_functions.R : noNA( dataset, column ) dataset: dataframe to eliminate NAs from column: column to check for NA values Gets rid of NA values… noNA &lt;- function(dataset, column){ return(dataset[!is.na(dataset[,column]),]) } Negative Values noNeg( dataset, column ) dataset: dataframe to eliminate negative values from column: column to check for negative values Gets rid of Negative values… noNeg &lt;- function(dataset, column){ return(dataset[which(dataset[,column] &gt; 0),]) } Outliers noOut( dataset, column ) dataset: dataframe to eliminate outliers from column: column to check for outliers Gets rid of Outliers… source(&quot;Functions/outlier_functions.R&quot;) noOut &lt;- function(dataset, column){ outliers &lt;- stdev_outliers(dataset, column) return(dataset[-outliers,]) } Standard Deviation (Lab Data Outliers) The noOut() function (above) calls the stdev_outliers() function (below), which is within outlier_functions.R. This outlier detection approach creates a PLS model, and identifies predictions that were that exceed a certain standard deviation threshold. {come back to for more detail} Executed within outlier_functions.R : stdev_outliers( dataset, column ) dataset: dataframe to eliminate standard deviation outliers from column: column to check for standard deviation outliers stdev_outliers &lt;- function(dataset, column){ # Create a PLS model with the data pls.fit &lt;- plsr(sqrt(get(column))~spc, ncomp= 20, data = dataset, valid=&quot;CV&quot;, segments = 50) #y, x, number components, data, cross validation, pred &lt;- c(predict(pls.fit, newdata = dataset$spc,ncomp=20))^2 # Identify outliers using a standard deviation threshold sd.outlier &lt;- optimum_sd_outlier(pred, dataset[,column], seq(0.1,3, by =0.02)) outlier.indices &lt;- outlier_indices(pred, dataset[,column], sd.outlier[1]) return(outlier.indices) } Fratio (Spectral Outliers) Script Sets the properties you would like make references sets for… # Remove rows with poor lab data properties &lt;- c(&quot;OC&quot;,&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) #Column names of lab data Gets rid of NA, negative and outlier values and saves subsets… for(property in properties){ prop_refset &lt;- all_refset prop_refset &lt;- noNA(prop_refset, property) # Remove NAs prop_refset &lt;- noNeg(prop_refset, property) # Remove Negative prop_refset &lt;- noOut(prop_refset, property) # Remove Outliers* savename &lt;- paste(&quot;refset&quot;, property, sep=&quot;.&quot;) # Ex: refset.OC assign(savename, prop_refset) save(list= savename, file=paste0(&quot;Data_Processed/&quot;, savename, &quot;.RData&quot;)) } Cal/Val Groups You may chose to subset a portion of the reference set as a calibration group which will be used to build the models- leaving the remaining samples as the validation set to test the model. Kennard Stone is a method for performing this type of separation while ensuring each group is representative of the set.4 Functions Executed within preprocess_functions.R : calValSplit( dataset, column ) dataset: dataframe- The reference dataset with a ‘spc’ column containing the spectral matrix library(prospectr) calValSplit &lt;- function(dataset){ #perform kennard stone to separate data into 80% calibration and 20% validation sets ken_stone&lt;- prospectr::kenStone(X = dataset$spc, k = as.integer(0.8*nrow(dataset)), metric = &quot;mahal&quot;, pc = 10) calib &lt;- dataset[ken_stone$model, ] valid &lt;- dataset[ken_stone$test, ] return(c(calib, valid)) } Script To split the reference set into calibration and validation groups… split &lt;- calValSplit(all_refset, property) calib &lt;- split[1]; valid &lt;- split[2] save(calib, file=&quot;Data_Processed/calib.ALL.RData&quot;) save(valid, file=&quot;Data_Processed/valid.ALL.RData&quot;) To split the property subsets into calibration and validation groups… properties &lt;- c(&quot;OC&quot;,&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) #Column names of lab data for(property in properties){ # Remove prop_refset &lt;- all_refset prop_refset &lt;- noNA(prop_refset, property) # Remove NAs prop_refset &lt;- noNeg(prop_refset, property) # Remove Negative prop_refset &lt;- noOut(prop_refset, property) # Remove Outliers* savename &lt;- paste(&quot;refset&quot;, property, sep=&quot;.&quot;) # Ex: refset.OC assign(savename, prop_refset) save(list= savename, file=paste0(&quot;Data_Processed/&quot;, savename, &quot;.RData&quot;)) # Split Calibration &amp; Validation Sets split &lt;- calValSplit(prop_refset, property) calib &lt;- split[1]; valid &lt;- split[2] save(calib, file=paste(&quot;Data_Processed/calib&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) save(valid, file=paste(&quot;Data_Processed/valid&quot;, property,&quot;RData&quot;, sep=&quot;.&quot;)) } prediction set preprocessing skips step 4.5 Select Calibration Set↩ https://github.com/philipp-baumann/simplerspec↩ sample_ids that are numeric may cause issues while merging so a string ID is advised↩ This step can be skipped if using MBL modeling approach which uses the entire dataset. If both modeling approaches are being used, you can load and row bind the calibration and validation sets for MBL↩ "],
["plsr-models.html", "5 PLSR Models 5.1 Model Theory 5.2 Making PLSR Models", " 5 PLSR Models 5.1 Model Theory Partial Least Squares Regression (PLSR) is a useful technique for making predictions on high dimensional datasets; Those with many columns or predictor variables relative to the number of rows or instances. For example, we are using 2720 columns of spectral data as predictor variables for only 333 rows of samples in the script to follow. A simple regression model would have the issue of overfitting and thus would not be suitable for this dataset. PLSR models, like Principal Component Analysis (PCA), reduce the dimensionality of the dataset by creating new set of orthogonal variables that explain the most variation in the data. However, instead of optimizing covariance amoung only the predictor variables, in this case the spectra, PLS optimizes covariance between the predictors and the response variable, the soil property of interest. To learn more about PLS, check out this youtube video: PLS Introductory Video 5.2 Making PLSR Models To predict using PLSR models, we use the pls package in r #---Packages---# library(pls) Assuming you have exited the environment where you pre-processed spectra, reload your calibration and validation sets. #---Load Data---# load(&quot;Single_Lib/calib.OC.RData&quot;) load(&quot;Single_Lib/valid.OC.RData&quot;) The plsr() command creates a model based on several inputs, outlined in the full documentation found here: https://www.rdocumentation.org/packages/pls/versions/2.7-2/topics/mvr and the manual, here: https://cran.r-project.org/web/packages/pls/pls.pdf Used in this example we have… Y The lab data/ observed data for the soil property you are trying to predict. We chose to square root transform this variable to normalize the data. Predictions made my the model are squared to back transform them. X A matrix of spectra with the same number of rows as Y ncomp The number of components that you would like to include in the model data The dataset containing Y and X valid The preferred validation type (“LOO”,“CV”,“none”) LOO for leave-one out CV for cross validation none if you chose not to include validation #---Create Model---# plsr.model &lt;- plsr(sqrt(get(property))~spc, ncomp=20, data = calib, valid=&quot;LOO&quot;) save(plsr.model, file = paste(&quot;plsr&quot;, property,&quot;.RData&quot;, sep=&quot;&quot;)) #saving the model #---Applying Model---# ncomp.onesigma &lt;- selectNcomp(plsr.model, method = &quot;onesigma&quot;, plot = TRUE, ylim = c(0, 50)) predVals &lt;- c(predict(plsr.model, newdata = predDat$spc, ncomp=ncomp.onesigma))^2 savename &lt;- paste(property, modelType, predDatName, paste(&quot;v&quot;,datname,sep=&quot;&quot;), sep=&quot;.&quot;) "],
["mbl-models.html", "6 MBL Models 6.1 Model Theory 6.2 Making MBL Predictions", " 6 MBL Models 6.1 Model Theory Overview Memory-Based Learning (MBL) is a local modeling approach that can be used to predict a given soil property from a set of spectral data, the prediction set. Like PLS, this approach relies on a reference set, containing both spectral data and known values for the soil property of interest (ie. Organic Carbon). While PLS create a single global model which can be applied to all samples in the prediction set, MBL makes a local model for each prediction. Local models are built from a sample’s nearest neighbors: samples in the reference set that are most similar to the sample being predicted. Similarity is measured by spectral similarity, which should reflect similarities in soil composition. Since each sample has a customized model, predictions are often more accurate than PLS predictions. However, MBL models can be quite computationally intensive since 1) A model is built for each sample being predicted 2) All samples in the prediction and reference set must be related in terms of similarity Animation The animation below illustrates how local modeling works in MBL. It is shown in multidimensional space since each spectral column is a dimension of the dataset. A Shows all the samples in the prediction set (red), overlaying all the samples in the reference set (gray) B Shows a circle indicating the nearest neighbors of a sample being predicted C Shows all the samples of the prediction set with their respective nearest neighbors D Shows how local models will be created for each prediction from these nearest neighbors Resemble Powerpoint: http://www.fao.org/fileadmin/user_upload/GSP/docs/Spectroscopy_dec13/SSW2013_f.pdf 6.2 Making MBL Predictions MBL Functions Full documentation on the mbl() function of the resemble package can be found below, or by typing ??resemble in r: MBL- https://www.rdocumentation.org/packages/resemble/versions/1.2.2/topics/mbl mbl(Yr, Xr, Yu = NULL, Xu, mblCtrl = mblControl(), dissimilarityM, group = NULL, dissUsage = &quot;predictors&quot;, k, k.diss, k.range, method, pls.c, pls.max.iter = 1, pls.tol = 1e-6, noise.v = 0.001, ...) MBL Control- https://www.rdocumentation.org/packages/resemble/versions/1.2.2/topics/mblControl mblControl(sm = &quot;pc&quot;, pcSelection = list(&quot;opc&quot;, 40), pcMethod = &quot;svd&quot;, ws = if(sm == &quot;movcor&quot;) 41, k0, returnDiss = FALSE, center = TRUE, scaled = TRUE, valMethod = c(&quot;NNv&quot;, &quot;loc_crossval&quot;), localOptimization = TRUE, resampling = 10, p = 0.75, range.pred.lim = TRUE, progress = TRUE, cores = 1, allowParallel = TRUE) Modeling Parameters This section explains some of the main ways to customize and optimize mbl models using mbl() in the resemble package. Below is an example workflow for modeling with MBL after the preprocessing steps have been completed: Input Datasets The mbl() function accepts 4 different data products, Xu Xr Yu and Yr, summarized in the table below: Both Xs are matrices with spectral data and both Ys are vectors with lab data for the property of interest. u indicates “uncertain” for our prediction set, and r indicates “reference” for our reference set. Yu is optional, since not all prediction sets will have associated lab data. If this is the case, set Yu to NULL. See the data preprocessing tab to prepare these datasets prior to modeling. In addition, it is necessary to remove all rows in the reference set inputs (Yr and Xr) that have NA values. If you would like to include Yu but there are missing values, you must also remove those rows in both prediction set inputs (Yu and Xu). Number of columns in Xr must equal that of Xu. Number of rows in Yr must equal that of Yu, if provided. Matrix of Spectral Neighbors When selecting nearest neighbors to build a local model, the mbl() function references a spectral dissimilarity matrix, which relates samples in the prediction and reference sets. This matrix can be created by setting the sm parameter in mblControl(), or can be passed into the mbl() function as dissimilarityM if a matrix has already been made. For creating the matrix, you will have to decide how spectral dissimilarity will be calculated by setting a couple variables in mblControl(): sm can be set to a variety of different methods for measuring distance in a multidimensional space. We have used &quot;pls&quot; &quot;pc&quot; &quot;euclid&quot; &quot;cosine&quot; &quot;cor&quot; and &quot;movcor&quot; pcSelection determines how the number of principal components will be chosen for calculating Mahalonobis dissimilarity (when sm = “pc”, “loc.pc”, “pls” or “loc.pls”) We have this set to the default options of (opc,40) meaning the optimal principal component method will be used and up to 40 components will be tested. . Lastly, you can specify how the matrix will be used within the local models, if at all, by setting the dissUsage parameter to &quot;weights&quot; &quot;predictors&quot; or &quot;none&quot;. If set to &quot;predictors&quot;, the column of the matrix which shows similarity to the sample being predicted, will be added as a predictor variable to build the local model. If set to &quot;weights&quot;, the neighbors are weighted based on dissimilarity/distance (those closer to the sample being predicted receive more weight in the model). . The matrix format will look like one of the following, depending on how it will be used… A. All reference and prediction sets samples as rows and columns (“predictors”) B. Reference set samples as rows, prediction set samples as columns (“weights”) Neighbor Selection The mbl() function allows you to specify how many nearest neighbors will be used to build local models, by setting either k, or k.diss and k.range. Option 1: Set k to a sequence of numbers to test, for how many neighbors to include. seq(40, 40, by=20) , would perform 1 iteration, using 40 nearest neighbors seq(40, 100, by=20), would perform 4 iterations, using 40, 60, 80 and 100 nearest neighbors Option 2: Set a dissimilarity threshold k.diss that limits the distance to search for neighbors from a sample. You can think of it as the radius of the circles shown in the model theory animation. Set k.range to the minimum and maximum number of neighbors you want to include, within the k.diss distance. Modeling Method Once neighbors are selected, MBL builds local models using the multivariate regression method specified with the variable method in the mbl() function. pls for partial least squares regression wapls1 for weighted average pls gpr for gaussian process with dot product covariance pls.c allows you to set the number of pls components to be used if either “pls” or “wasp1” is used. A single number if pls is used A vector containing the minimum and maximum number of components to be used, if wasp1 is used Validation Method You can specify the validation method by setting the parameter valMethod within the mblControl() function. NNv for leave-nearest-neighbour-out cross validation loc_crossval for local leave group out cross validation none If you chose not to validate the model. This will improve processing speed. Other Should I include this section? center scaled Sample Code Define Input Data Xu &lt;- predDat$spc Yu &lt;- sqrt(predDat[,property]) Yr &lt;- sqrt(calib[,property]) Xr &lt;- calib$spc Xu &lt;- Xu[!is.na(Yu),] Yu &lt;- Yu[!is.na(Yu)] Xr &lt;- Xr[!is.na(Yr),] Yr &lt;- Yr[!is.na(Yr)] Example 1 ctrl &lt;- mblControl(sm = &#39;pc&#39;, pcSelection = list(&#39;opc&#39;, 50), valMethod = &#39;loc_crossval&#39;,center=TRUE,scale=FALSE,allowParallel=FALSE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Yu = Yu, Xu = Xu, mblCtrl = ctrl, dissUsage = &#39;none&#39;, k = seq(40, 100, by = 20), method = &#39;pls&#39;, pls.c = 6) predVals &lt;- c(mbl.sqrt$results$Nearest_neighbours_40$pred)^2 Example 2 diss2test &lt;- seq(0.3, 1, by=0.1) kminmax &lt;- c(10, nrow(calib$spc)) pls.f &lt;- c(minpls=3, maxpls=20) ctrl &lt;- mblControl(sm = pls, pcSelection = list(&quot;opc&quot;, 50), valMethod = &quot;NNv&quot;, returnDiss = TRUE, scaled = FALSE, center = TRUE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Xu = Xu, mblCtrl = ctrl, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = &quot;wapls1&quot;) idx.best.ca &lt;- which.min(mbl.sqrt$nnValStats$st.rmse) best.kdiss.ca &lt;- mbl.sqrt$nnValStats$k.diss[idx.best.ca] predVals &lt;- c(getPredictions(mbl.sqrt)[, idx.best.ca])^2 Model Output Cross Validation Statistics mbl.sqrt$localCrossValStats index_best_model &lt;- which.min(mbl.sqrt$localCrossValStats$st.rmse) Prediction Statistics If Yu, lab data for the prediction set is provided, it will calculate how well the models predicted in terms of rmse, st.rmse and r2 mbl.sqrt$YuPredictionStats "],
["model-performance.html", "7 Model Performance 7.1 Statistics 7.2 Plots 7.3 Outliers", " 7 Model Performance 7.1 Statistics Create summary table with the predictions against the lab data #---Summary Table---# col.names &lt;- colnames(predDat) propCol &lt;- which(col.names == toString(property))[1] pred_obs &lt;- data.frame(predDat[,1], predVals, (predDat[,propCol])*unit_adj) names(pred_obs) &lt;- c(&quot;ID&quot;, &quot;pred&quot;, &quot;obs&quot;) 7.2 Plots #---Validation Plot---# max &lt;- max(pred_obs[,c(&quot;pred&quot;, &quot;obs&quot;)]) plot.plsr(pred_obs$obs, pred_obs$pred, property, c(0,(1.1*max)),units) 7.3 Outliers 7.3.1 Standard Deviation 7.3.2 F-Ratio "],
["references.html", "References", " References "]
]
