[
["index.html", "Soil Predictions using MIR-Spectroscopy 1 Introduction", " Soil Predictions using MIR-Spectroscopy Charlotte Rivard 5/8/2020 1 Introduction This guide documents the use of MIR-spectroscopy at the Woods Hole Research Center, to predict various soil health properties. It is aimed at helping other teams build soil spectral libraries, create models from their data, access model performance against known values, and ultimately make predictions on new data. The machine learning methods outlined in this document were selected after assessing the performance of a variety of models. This process is explained in the following paper: Dangal S.R.S., J. Sanderman, S. Wills, and L. Rameriz-Lopez. 2019. Accurate and Precise Prediction of Soil Properties from a Large Mid-Infrared Spectral Library. Soil Systems 3(1):11. doi:10.3390/soilsystems3010011 Questions and comments can be sent to jsanderman@whrc.org or crivard@whrc.org Woods Hole Research Center Website: whrc.org Instagram: woodsholeresearchcenter Address: 149 Woods Hole Rd, Falmouth, MA 02540 "],
["background.html", "2 Background", " 2 Background Mid-Infrared (MIR) spectroscopy is a quick, cost effective technique for predicting a variety of soil health properties. “Diffuse reflectance spectroscopy (DRS) has been demonstrated to be a viable alternative for rapidly characterizing and measuring soil properties compared with time-consuming and expensive conventional soil laboratory analysis [6]. Visible (vis; 400–700 nm), near-infrared (NIR; 700–2500 nm), and mid-infrared (MIR; 2500–25,000 nm) regions have been used widely to characterize soil minerals and organic matter at the global [5,7,8], regional [9,10], national [11,12] and local scale [13]. Vis–NIR can be preferable to MIR due to its low instrumental cost and potential for field deployment [14]. Therefore, for soil surveys that require high sampling density with basic soil properties measurement (for, e.g., organic carbon, soil texture), vis–NIR may be preferable. In contrast, the MIR region contains strong molecular vibrations of most important soil minerals and organic components [7,15]. As a result, models built using MIR databases often perform better compared with the vis–NIR database for many soil properties, particularly for more minor soil constituents [6,16]” (soil systems paper) https://www.researchgate.net/figure/Representative-soil-mid-IR-spectrum-showing-absorptions-related-to-the-mineral-and_fig1_282420764 "],
["code-overview.html", "3 Code Overview 3.1 File Structure 3.2 Required Packages", " 3 Code Overview 3.1 File Structure 3.2 Required Packages "],
["data-preprocessing.html", "4 Data Preprocessing 4.1 Extract OPUS files 4.2 Process Spectra 4.3 Merge with Lab Data 4.4 Select Calibration Set", " 4 Data Preprocessing 4.1 Extract OPUS files For Bruker Instruments, an OPUS file containing spectral data, will be output for each sample that is scanned. To compile these separate files into one dataset, we use a couple functions from the ‘simplerspec’ package by Philip Baumann, as well as the stringr and foreach packages. #---Packages---# library(stringr) #used for str_sub library(foreach) #used within read-opus-universal.R source(&quot;Single_Lib/reference_files/gather-spc.R&quot;) #simplerspec function source(&quot;Single_Lib/reference_files/read-opus-universal.R&quot;) #simplerspec function Gets the paths of all OPUS files… #---List Files---# spectraPath &lt;- &quot;/Single_Lib/SPECTRA&quot; #folder of OPUS files dirs &lt;- list.dirs(paste(getwd(),spectraPath,sep=&quot;&quot;), full.names=TRUE) all.files &lt;- list.files(dirs, pattern= &quot;*.0&quot;, recursive=TRUE,full.names=TRUE) Extracts the spectra and gathers it into a tibble data frame… #---Extract Spectra---# spc_list &lt;- read_opus_univ(fnames = all.files, extract = c(&quot;spc&quot;)) soilspec_tbl &lt;- spc_list %&gt;% gather_spc() spc &lt;- soilspec_tbl$spc Optionally truncates the dataset to ensure the spectra from different samples align. Only necessary if instrument settings are changed between runs. 3017 would be changed to the number of spectral columns (wavelengths collected) spc &lt;- lapply(1:length(spc),function(x) spc[[x]][,1:3017]) Processes spectra into a dataframe and assigns a sample_id, based off the file names… *sample_ids that are numeric may cause issues while merging so a string ID is advised. spc.df &lt;- as.data.frame(matrix(unlist(spc), nrow=length(spc), byrow=T)) colnames(spc.df) &lt;- colnames(spc[[1]]) spc.df &lt;- data.frame(sample_id = soilspec_tbl$sample_id, spc.df) spc.df$sample_id &lt;- str_sub(spc.df$sample_id,1,9) Optionally saves the spectra as an R dataset or csv file… save(spc.df, file=&quot;spectra_original.RData&quot;) write.csv(spc.df, &quot;spectra_original.csv&quot;) 4.2 Process Spectra We narrow down the regions of the spectra by truncating wavenumbers below 628 and between 2268 to 2389, which is a CO2 sensitive region. #---Edit Spectral Columns---# col.names &lt;- colnames(spectra$spc) #get column names which are wavenumbers col.names &lt;- as.numeric(substring(col.names,2)) cutoff &lt;- which(col.names &lt;= 628)[1] spectra$spc &lt;- spectra$spc[,-c(cutoff:length(col.names))] #truncate at &gt;= 628 min.index &lt;- which(col.names &lt;= 2389)[1] max.index &lt;- which(col.names &lt;= 2268)[1] spectra$spc &lt;- spectra$spc[,-c(min.index:max.index)] #remove CO2 region We perform a baseline transformation to normalize the spectra #---Baseline Transformation---# library(matrixStats) base_offset &lt;- function(x){ test &lt;- rowMins(x) return(x-test) } spectra$spc &lt;- base_offset(spectra$spc) Optionally saves the processed spectra as an R dataset or csv file… #---Save Spectra---# save(spc.df, file=&quot;Single_Lib/spectra_processed.RData&quot;) write.csv(spc.df, &quot;Single_Lib/spectra_processed.csv&quot;) 4.3 Merge with Lab Data If there is lab data associated with your soil samples, this can be merged with the spectral data and later used to assess the performance of your models.The example lab dataset below provides information about where the soil sample was taken with the Site_ID and Horizon, as well as the lab measurements for various soil properties including Organic Carbon, Sand, Silt and Clay. #---Read Lab Data---# library(readr) #used to open the .csv file lab &lt;- data.frame(read_csv(&quot;Single_Lib/LAB_DATA.csv&quot;, col_types=cols())) #read in the lab data The merge() command joins the lab dataset to the spectral dataset. The all.y=TRUE parameter indicates that the final dataset will contain all the rows of spectra. This means that if some samples do not have lab data, they will be assigned a value of NA but the spectra will remain in the set. #---Merge Data---# all_data &lt;- merge(lab, spectra, all.y=TRUE) save(all_data, file=&quot;Single_Lib/spectra_lab_merge.RData&quot;) write.csv(all_data, &quot;Single_Lib/spectra_lab_merge.csv&quot;, row.names=FALSE) The final dataframe contains a unique ID, lab data, and a matrix of spectral data called ‘spc’. It is suggested to save this file as RData so it may be reloaded as needed. 4.4 Select Calibration Set {OPTIONAL} Once the full dataset is processed, it must be split into calibration and validation sets to that will be used to train and test the model. This process must be repeated for each soil property that is being predicted, since outliers and NA values may vary among the properties. The example below shows this step for a single property, while the RUNFILE code shows it implemented in a loop for several properties. #---Packages---# library(pls) #used for plsr model source(&quot;Single_Lib/reference_files/functions_modelChoice.R&quot;) #used for optimum_sd_outlier() Rows that have NA or negative values for the property being predicted (% Organic Carbon) are excluded from the dataset. #---Eliminate NA &amp; negative---# all_data &lt;- all_data[!is.na(all_data[,property]),] #no NAs all_data &lt;- all_data[which(all_data[,property] &gt; 0),] #no negative values A pls model is created from the remaining data to flag outliers. Samples that have inaccurate predictions are excluded. This is measured by regressing the predictions against its associated lab data, and evaluating how many standard deviations each sample is from the best fit line. Those &gt;99th percentile are removed since… #---Remove Outliers---# pls.fit &lt;- plsr(sqrt(get(property))~spc, ncomp= 20, data = all_data, valid=&quot;CV&quot;, segments = 50) #fit a pls model to the data pred &lt;- c(predict(pls.fit, newdata = all_data$spc,ncomp=20))^2 #make predictions sd.outlier &lt;- optimum_sd_outlier(pred, all_data[,property], seq(0.1,3, by =0.02)) #flag samples that performed poorly as outliers row.index &lt;- outlier(pred, all_data[,property], sd.outlier[1]) if(length(row.index) &gt; 0){ all_data &lt;- all_data[row.index,] #subset non-outliers } Performing kennard stone to separate data into 80% calibration and 20% validation sets. This step can be skipped if using MBL modeling approach which uses the entire dataset. If both modeling approaches are being used, you can load and row bind the calibration and validation sets for MBL. #---Kennard Stone---# ken_stone&lt;- prospectr::kenStone(X = all_data$spc, k = as.integer(0.8*nrow(all_data)), metric = &quot;mahal&quot;, pc = 10) calib &lt;- all_data[ken_stone$model, ] #subset calibration set valid &lt;- all_data[ken_stone$test, ] #subset validation set Save calibration and validation set for that particular property #Save for property save(calib, file=paste(&quot;Single_Lib/calib&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;)) save(valid, file=paste(&quot;Single_Lib/valid&quot;,property,&quot;RData&quot;, sep=&quot;.&quot;)) An alternative to saving the datasets would be to add a column indicating whether a sample falls under the calibration or validation set. This could be used to subset the correct datasets when making predictions. If the full dataset is large and takes a while to load, saving the former approach is preferred. If the set loads relatively quickly, it may be worth it to save storage space and use the latter method. "],
["plsr-models.html", "5 PLSR Models 5.1 Model Theory 5.2 Making PLSR Models", " 5 PLSR Models 5.1 Model Theory Partial Least Squares Regression (PLSR) is a useful technique for making predictions on high dimensional datasets; Those with many columns or predictor variables relative to the number of rows or instances. For example, we are using 2720 columns of spectral data as predictor variables for only 333 rows of samples in the script to follow. A simple regression model would have the issue of overfitting and thus would not be suitable for this dataset. PLSR models, like Principal Component Analysis (PCA), reduce the dimensionality of the dataset by creating new set of orthogonal variables that explain the most variation in the data. However, instead of optimizing covariance amoung only the predictor variables, in this case the spectra, PLS optimizes covariance between the predictors and the response variable, the soil property of interest. To learn more about PLS, check out this youtube video: PLS Introductory Video 5.2 Making PLSR Models To predict using PLSR models, we use the pls package in r #---Packages---# library(pls) Assuming you have exited the environment where you pre-processed spectra, reload your calibration and validation sets. #---Load Data---# load(&quot;Single_Lib/calib.OC.RData&quot;) load(&quot;Single_Lib/valid.OC.RData&quot;) The plsr() command creates a model based on several inputs, outlined in the full documentation found here: https://www.rdocumentation.org/packages/pls/versions/2.7-2/topics/mvr and the manual, here: https://cran.r-project.org/web/packages/pls/pls.pdf Used in this example we have… Y The lab data/ observed data for the soil property you are trying to predict. We chose to square root transform this variable to normalize the data. Predictions made my the model are squared to back transform them. X A matrix of spectra with the same number of rows as Y ncomp The number of components that you would like to include in the model data The dataset containing Y and X valid The preferred validation type (“LOO”,“CV”,“none”) LOO for leave-one out CV for cross validation none if you chose not to include validation #---Create Model---# plsr.model &lt;- plsr(sqrt(get(property))~spc, ncomp=20, data = calib, valid=&quot;LOO&quot;) save(plsr.model, file = paste(&quot;plsr&quot;, property,&quot;.RData&quot;, sep=&quot;&quot;)) #saving the model #---Applying Model---# ncomp.onesigma &lt;- selectNcomp(plsr.model, method = &quot;onesigma&quot;, plot = TRUE, ylim = c(0, 50)) predVals &lt;- c(predict(plsr.model, newdata = predDat$spc, ncomp=ncomp.onesigma))^2 savename &lt;- paste(property, modelType, predDatName, paste(&quot;v&quot;,datname,sep=&quot;&quot;), sep=&quot;.&quot;) "],
["mbl-models.html", "6 MBL Models 6.1 Model Theory 6.2 Making MBL Predictions", " 6 MBL Models 6.1 Model Theory Overview Memory-Based Learning (MBL) is a local modeling approach that can be used to predict a given soil property from a set of spectral data, the prediction set. Like PLS, this approach relies on a reference set, containing both spectral data and known values for the soil property of interest (ie. Organic Carbon). While PLS create a single global model which can be applied to all samples in the prediction set, MBL makes a local model for each prediction. Local models are built from a sample’s nearest neighbors: samples in the reference set that are most similar to the sample being predicted. Similarity is measured by spectral similarity, which should reflect similarities in soil composition. Since each sample has a customized model, predictions are often more accurate than PLS predictions. However, MBL models can be quite computationally intensive since 1) A model is built for each sample being predicted 2) All samples in the prediction and reference set must be related in terms of similarity Animation The animation below illustrates how local modeling works in MBL. It is shown in multidimensional space since each spectral column is a dimension of the dataset. A Shows all the samples in the prediction set (red), overlaying all the samples in the reference set (gray) B Shows a circle indicating the nearest neighbors of a sample being predicted C Shows all the samples of the prediction set with their respective nearest neighbors D Shows how local models will be created for each prediction from these nearest neighbors Resemble Powerpoint: http://www.fao.org/fileadmin/user_upload/GSP/docs/Spectroscopy_dec13/SSW2013_f.pdf 6.2 Making MBL Predictions MBL Functions Full documentation on the mbl() function of the resemble package can be found below, or by typing ??resemble in r: MBL- https://www.rdocumentation.org/packages/resemble/versions/1.2.2/topics/mbl mbl(Yr, Xr, Yu = NULL, Xu, mblCtrl = mblControl(), dissimilarityM, group = NULL, dissUsage = &quot;predictors&quot;, k, k.diss, k.range, method, pls.c, pls.max.iter = 1, pls.tol = 1e-6, noise.v = 0.001, ...) MBL Control- https://www.rdocumentation.org/packages/resemble/versions/1.2.2/topics/mblControl mblControl(sm = &quot;pc&quot;, pcSelection = list(&quot;opc&quot;, 40), pcMethod = &quot;svd&quot;, ws = if(sm == &quot;movcor&quot;) 41, k0, returnDiss = FALSE, center = TRUE, scaled = TRUE, valMethod = c(&quot;NNv&quot;, &quot;loc_crossval&quot;), localOptimization = TRUE, resampling = 10, p = 0.75, range.pred.lim = TRUE, progress = TRUE, cores = 1, allowParallel = TRUE) Modeling Parameters This section explains some of the main ways to customize and optimize mbl models using mbl() in the resemble package. Below is an example workflow for modeling with MBL: Input Datasets The mbl() function accepts 4 different data products, Xu Xr Yu and Yr, summarized in the table below: Both Xs are matrices with spectral data and both Ys are vectors with lab data for the property of interest. u indicates “uncertain” for our prediction set, and r indicates “reference” for our reference set. Yu is optional, since not all prediction sets will have associated lab data. If this is the case, set Yu to NULL. See the data preprocessing tab to prepare these datasets prior to modeling. In addition, it is necessary to remove all rows in the reference set inputs (Yr and Xr) that have NA values. If you would like to include Yu but there are missing values, you must also remove those rows in both prediction set inputs (Yu and Xu). Number of columns in Xr must equal that of Xu. Number of rows in Yr must equal that of Yu, if provided. Matrix of Spectral Neighbors When selecting nearest neighbors to build a local model, the mbl() function references a spectral dissimilarity matrix, which relates samples in the prediction and reference sets. This matrix can be created by setting the sm parameter in mblControl(), or can be passed into the mbl() function as dissimilarityM if a matrix has already been made. For creating the matrix, you will have to decide how spectral dissimilarity will be calculated by setting a couple variables in mblControl(): sm can be set to a variety of different methods for measuring distance in a multidimensional space. We have used &quot;pls&quot; &quot;pc&quot; &quot;euclid&quot; &quot;cosine&quot; &quot;cor&quot; and &quot;movcor&quot; pcSelection determines how the number of principal components will be chosen for calculating Mahalonobis dissimilarity (when sm = “pc”, “loc.pc”, “pls” or “loc.pls”) We have this set to the default options of (opc,40) meaning the optimal principal component method will be used and up to 40 components will be tested. . Lastly, you can specify how the matrix will be used within the local models, if at all, by setting the dissUsage parameter to &quot;weights&quot; &quot;predictors&quot; or &quot;none&quot;. If set to &quot;predictors&quot;, the column of the matrix which shows similarity to the sample being predicted, will be added as a predictor variable to build the local model. If set to &quot;weights&quot;, the neighbors are weighted based on dissimilarity/distance (those closer to the sample being predicted receive more weight in the model). . The matrix format will look like one of the following, depending on how it will be used… A. All reference and prediction sets samples as rows and columns (“predictors”) B. Reference set samples as rows, prediction set samples as columns (“weights”) Neighbor Selection The mbl() function allows you to specify how many nearest neighbors will be used to build local models, by setting either k, or k.diss and k.range. Option 1: Set k to a sequence of numbers to test, for how many neighbors to include. seq(40, 40, by=20) , would perform 1 iteration, using 40 nearest neighbors seq(40, 100, by=20), would perform 4 iterations, using 40, 60, 80 and 100 nearest neighbors Option 2: Set a dissimilarity threshold k.diss that limits the distance to search for neighbors from a sample. You can think of it as the radius of the circles shown in the model theory animation. Set k.range to the minimum and maximum number of neighbors you want to include, within the k.diss distance. Modeling Method Once neighbors are selected, MBL builds local models using the multivariate regression method specified with the variable method in the mbl() function. pls for partial least squares regression wapls1 for weighted average pls gpr for gaussian process with dot product covariance pls.c allows you to set the number of pls components to be used if either “pls” or “wasp1” is used. A single number if pls is used A vector containing the minimum and maximum number of components to be used Validation Method You can specify the validation method by setting the parameter valMethod within the mblControl() function. NNv for leave-nearest-neighbour-out cross validation loc_crossval for local leave group out cross validation none If you chose not to validate the model. This will improve processing speed. Other Should I include this section? center scaled Sample Code Define Input Data Xu &lt;- predDat$spc Yu &lt;- sqrt(predDat[,property]) Yr &lt;- sqrt(calib[,property]) Xr &lt;- calib$spc Xu &lt;- Xu[!is.na(Yu),] Yu &lt;- Yu[!is.na(Yu)] Xr &lt;- Xr[!is.na(Yr),] Yr &lt;- Yr[!is.na(Yr)] Example 1 ctrl &lt;- mblControl(sm = &#39;pc&#39;, pcSelection = list(&#39;opc&#39;, 50), valMethod = &#39;loc_crossval&#39;,center=TRUE,scale=FALSE,allowParallel=FALSE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Yu = Yu, Xu = Xu, mblCtrl = ctrl, dissUsage = &#39;none&#39;, k = seq(40, 100, by = 20), method = &#39;pls&#39;, pls.c = 6) predVals &lt;- c(mbl.sqrt$results$Nearest_neighbours_40$pred)^2 Example 2 diss2test &lt;- seq(0.3, 1, by=0.1) kminmax &lt;- c(10, nrow(calib$spc)) pls.f &lt;- c(minpls=3, maxpls=20) ctrl &lt;- mblControl(sm = pls, pcSelection = list(&quot;opc&quot;, 50), valMethod = &quot;NNv&quot;, returnDiss = TRUE, scaled = FALSE, center = TRUE) mbl.sqrt &lt;- mbl(Yr = Yr, Xr = Xr, Xu = Xu, mblCtrl = ctrl, dissUsage = &quot;none&quot;, k.diss = diss2test, k.range = kminmax, pls.c = pls.f, method = &quot;wapls1&quot;) idx.best.ca &lt;- which.min(mbl.sqrt$nnValStats$st.rmse) best.kdiss.ca &lt;- mbl.sqrt$nnValStats$k.diss[idx.best.ca] predVals &lt;- c(getPredictions(mbl.sqrt)[, idx.best.ca])^2 "],
["model-performance.html", "7 Model Performance 7.1 Statistics 7.2 Plots", " 7 Model Performance 7.1 Statistics Create summary table with the predictions against the lab data #---Summary Table---# col.names &lt;- colnames(predDat) propCol &lt;- which(col.names == toString(property))[1] pred_obs &lt;- data.frame(predDat[,1], predVals, (predDat[,propCol])*unit_adj) names(pred_obs) &lt;- c(&quot;ID&quot;, &quot;pred&quot;, &quot;obs&quot;) 7.2 Plots #---Validation Plot---# max &lt;- max(pred_obs[,c(&quot;pred&quot;, &quot;obs&quot;)]) plot.plsr(pred_obs$obs, pred_obs$pred, property, c(0,(1.1*max)),units) "],
["references.html", "References", " References "]
]
